{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Analysis\n",
    "_**HARD: This is a curveball assignment. Plus, this is Python without Pandas.**_\n",
    "\n",
    "#### The objective of this assignment is for you to explain what is happening in each cell in clear, understandable language. \n",
    "\n",
    "#### _There is no need to code._ The code is there for you, and it already runs. Your task is only to explain what each line in each cell does.\n",
    "\n",
    "#### The placeholder cells should describe what happens in the cell below it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below imports `os` as a dependency because the `os.path.join` function. Also, the `string` dependency is needed because later in the script, `string.punctuation` will be used to detect and remove punctuation symbols. `from collection import Counter` will allow us to count items and store then as a dictionary with the element being stored as keys and the counts of those elements being stored as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are setting the path for the resume file and using join because its in the same folder as our notebook file. We then create sets as denoted by {} containg the required and desired skills we will be searching for in the resume. Creating them as sets will allow us to perform intersections with the words in our resume files to be able to quickly identify which of the identified words appear in our resume file. These sets are in all CAPS and seperated by underscores because they are constants which will be used throughout the module and remain unchanged. This is the formatting desgniated by the python style guide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "resume_path = os.path.join(\".\", 'resume.md')\n",
    "\n",
    "# Skills to match\n",
    "REQUIRED_SKILLS = {\"excel\", \"python\", \"mysql\", \"statistics\"}\n",
    "DESIRED_SKILLS = {\"r\", \"git\", \"html\", \"css\", \"leaflet\", \"modeling\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell is defining a function to read in a file and make it all lowercase and then split all of the words into tokens (saves the words as individual values and gets rid of the spaces) and store them as a list. Now that the function is defined we can use it anywhere in this module to read in files and store the contents as a tokenized list. All we have to do it call the function \"load_file\" and pass it our resume_path variable, then it will execute the steps we setup in the with statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    # Helper function to read a file and return the data.\n",
    "    with open(filepath, \"r\") as resume_file_handler:\n",
    "        resume_contents = resume_file_handler.read()\n",
    "        resume_contents = resume_contents.lower()\n",
    "        resume_tokens = resume_contents.split()\n",
    "        return resume_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is calling the load_file function we created in the previous cell and passing it the resume_file path we set in the first cell then setting the results of the function to the variable \"word_list\". Word_list now contains a list of all the tokens (groups of text from the resume file as seperated by spaces, this could be one character or symbol or multiple charaters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the text for a Resume\n",
    "word_list = load_file(resume_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we first create an empty set named \"resume\". We then use a for statement to iterate through each element or token in the word_list we created in the previous cell and then add it to the \"resume\" set. This creates a set which we can then use to intersect with our other sets we created. After running the for loop we are printing out the tokens in our resume set. Next we create a set containing punctuation symbols which we imported from string. Creating a set containing punctuation will allow us to perform a left join keeping only the elements which exist in the resume set and removing elements which exist in both the resume and punctuation set. We then print out \"resume\" again which now has the tokens which matched exactly to punctuation removed, such as the single '#' value. \n",
    "\n",
    "'\\n' tells the print function to enter an empty line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORDS BEFORE MOVING PUNCTUATION\n",
      "{'pandas', 'analytics', 'html/css,', 'frank', 'databases', 'to', 'apps', 'data', 'developing', 'contributing', 'graduate', 'software', '*', 'in', 'forecasting', 'cloud', 'statistics', 'social', 'advanced', 'mining,', 'html,', '##', 'hadoop', 'files', 'intelligence', 'visualization', 'working', 'modeling', 'apis.', 'mysql', 'boot', 'mongodb', 'r,', 'big', 'basic', 'creating', 'd3', 'n.', 'using', 'machine', 'pivot', 'scripts', 'tableau,', 'visualizations', 'd3,', 'css,', 'excel.', 'skills', 'interests', 'hadoop,', 'experience', 'learning', 'from', 'microsoft', 'python', 'analyze', 'tableau', 'mining', 'the', 'python,', 'excel,', 'git/github', 'learning,', 'bootstrap,', '#', 'and', 'statistics,', 'performing', 'sql,', 'designing', 'sets', 'interactions,', 'education', 'open-source', 'vba', 'stein', 'media', 'api', 'writing', 'front-end', 'leaflet.js', 'web', 'algorithms', 'aws', 'tables', 'with', 'camp', 'business', 'javascript,'}\n",
      "\n",
      "WORDS AFTER MOVING PUNCTUATION\n",
      "{'pandas', 'analytics', 'html/css,', 'frank', 'databases', 'to', 'apps', 'data', 'developing', 'contributing', 'graduate', 'software', 'in', 'forecasting', 'cloud', 'statistics', 'social', 'advanced', 'mining,', 'html,', '##', 'hadoop', 'files', 'intelligence', 'visualization', 'working', 'modeling', 'apis.', 'mysql', 'boot', 'mongodb', 'r,', 'big', 'basic', 'creating', 'd3', 'n.', 'using', 'machine', 'pivot', 'scripts', 'tableau,', 'visualizations', 'd3,', 'css,', 'excel.', 'skills', 'interests', 'hadoop,', 'experience', 'learning', 'from', 'microsoft', 'python', 'analyze', 'tableau', 'mining', 'the', 'python,', 'excel,', 'git/github', 'learning,', 'bootstrap,', 'and', 'statistics,', 'performing', 'sql,', 'designing', 'sets', 'interactions,', 'education', 'open-source', 'vba', 'stein', 'media', 'api', 'writing', 'front-end', 'leaflet.js', 'web', 'algorithms', 'aws', 'tables', 'with', 'camp', 'business', 'javascript,'}\n"
     ]
    }
   ],
   "source": [
    "# Create a set of unique words from the resume\n",
    "resume = set()\n",
    "\n",
    "# HINT: Single elements in a programming language are often referred to as tokens\n",
    "for token in word_list:\n",
    "    resume.add(token)\n",
    "\n",
    "print('\\nWORDS BEFORE MOVING PUNCTUATION')    \n",
    "print(resume)\n",
    "\n",
    "# Remove Punctuation that were read as whole words\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "# HINT: Attributes that are in `resume` that are not in `punctuation` (difference)\n",
    "resume = resume - punctuation\n",
    "\n",
    "\n",
    "\n",
    "print('\\nWORDS AFTER MOVING PUNCTUATION')    \n",
    "print(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we are using set intersection to perform an inner join between the Required Skills set and the resume set as well as the Desired Skills set and the resume set, then print out the results. Doing an inner join on each of these two sets will return words which only exist in both sets. This allows us to quickly determine which of our required and desired skills are contained in the resume. We then use list comprehension to iterate through the wordlist to remove additional punctuation. First we are iterating through the word list element by element and checking to see if its in the punctuation, if it is not in punctuation we add it to word_list. We then go through the same process but iterate through each characted in each element which allows us to remove punctuation anywhere in the element. After we have removed all the punctuation we create a list of stop words and itterate through the word list and remove any words which are contained in our stop_words list. Its important to note that each time we are performing these for loops we are reseting word_list to contain our new list with the punctuation and stop words removed. Since our stop words are stored in a list we could add more elements if we wanted to and the module would still run correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUIRED SKILLS\n",
      "{'python', 'statistics', 'mysql'}\n",
      "DESIRED SKILLS\n",
      "{'modeling'}\n",
      "\n",
      "WORD LIST AFTER PUNCTUATION REMOVAL\n",
      "['frank', 'n.', 'stein', '##', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '##', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel.', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis.', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'front-end', 'web', 'visualizations', 'using', 'html,', 'css,', 'bootstrap,', 'd3,', 'and', 'leaflet.js', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '##', 'skills', 'microsoft', 'excel,', 'python,', 'javascript,', 'html/css,', 'api', 'interactions,', 'social', 'media', 'mining,', 'sql,', 'hadoop,', 'tableau,', 'advanced', 'statistics,', 'machine', 'learning,', 'r,', 'git/github', '##', 'interests', 'contributing', 'to', 'open-source', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html,', 'css,', 'javascript,', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER CHARACTER PUNCTUATION REMOVAL\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'and', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'and', 'vba', 'scripts', 'in', 'excel', 'modeling', 'and', 'forecasting', 'data', 'using', 'basic', 'statistics', 'writing', 'python', 'scripts', 'to', 'analyze', 'data', 'sets', 'from', 'files', 'and', 'apis', 'social', 'media', 'mining', 'using', 'python', 'working', 'with', 'mysql', 'and', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'using', 'html', 'css', 'bootstrap', 'd3', 'and', 'leafletjs', 'using', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'with', 'hadoop', 'working', 'with', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'to', 'opensource', 'software', 'data', 'analytics', 'with', 'python', 'and', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'with', 'html', 'css', 'javascript', 'and', 'd3', 'working', 'with', 'big', 'data', 'in', 'the', 'cloud', 'using', 'aws']\n",
      "\n",
      "WORD LIST AFTER STOP WORDS\n",
      "['frank', 'n', 'stein', '', 'education', 'data', 'analytics', 'visualization', 'boot', 'camp', 'graduate', '', 'experience', 'creating', 'pivot', 'tables', 'vba', 'scripts', 'excel', 'modeling', 'forecasting', 'data', 'basic', 'statistics', 'writing', 'python', 'scripts', 'analyze', 'data', 'sets', 'from', 'files', 'apis', 'social', 'media', 'mining', 'python', 'mysql', 'mongodb', 'databases', 'developing', 'frontend', 'web', 'visualizations', 'html', 'css', 'bootstrap', 'd3', 'leafletjs', 'the', 'tableau', 'business', 'intelligence', 'software', 'performing', 'big', 'data', 'analytics', 'hadoop', 'machine', 'learning', 'algorithms', '', 'skills', 'microsoft', 'excel', 'python', 'javascript', 'htmlcss', 'api', 'interactions', 'social', 'media', 'mining', 'sql', 'hadoop', 'tableau', 'advanced', 'statistics', 'machine', 'learning', 'r', 'gitgithub', '', 'interests', 'contributing', 'opensource', 'software', 'data', 'analytics', 'python', 'pandas', 'designing', 'data', 'visualization', 'web', 'apps', 'html', 'css', 'javascript', 'd3', 'big', 'data', 'the', 'cloud', 'aws']\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Required Skills Match using Set Intersection\n",
    "print('REQUIRED SKILLS')\n",
    "print(resume & REQUIRED_SKILLS)\n",
    "\n",
    "# Calculate the Desired Skills Match using Set Intersection\n",
    "print('DESIRED SKILLS')\n",
    "print(resume & DESIRED_SKILLS)\n",
    "\n",
    "\n",
    "# Word Punctuation Cleaning\n",
    "word_list = [word for word in word_list if word not in string.punctuation]\n",
    "print('\\nWORD LIST AFTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Character Punctuation Cleaning\n",
    "word_list = [''.join(char for char in word if char not in string.punctuation) for word in word_list]\n",
    "print('\\nWORD LIST AFTER CHARACTER PUNCTUATION REMOVAL')\n",
    "print(word_list)\n",
    "\n",
    "# Clean Stop Words\n",
    "stop_words = [\"and\", \"with\", \"using\", \"##\", \"working\", \"in\", \"to\"]\n",
    "word_list = [word for word in word_list if word not in stop_words]\n",
    "print('\\nWORD LIST AFTER STOP WORDS')\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we are first creating a dictionary named \"word_count\" and populating the keys with each element from our word_list and then assigning a value to each of those keys as 0. We then run a for loop and iterate through each word in word list and add 1 to the corresponding key value pair in our word_count dictionary. For example, the for loop gets the value 'data' from word list, it then goes to the word_count dictionary and adds one to the value corresponding to the key \"data\". When the for loop gets to 'data' in word list again it will add 1 to the dictionary value again mking it 2. It will continue this process for each word in word_list. \n",
    "\n",
    "Next we are using the counter function to perform the same funtion as our for loop did. The difference here is we dont have to pass it a dictionary because counter will create a dicitonary as it iterates through word_list. After performing the counter function we are removing the \"\" value from our word_count dictionary and then comparing it to the word_counter dictionary created by the counter function. These two dictionaries are compared to see if they match and fail because the \"\" value was removed from word_count. \n",
    "\n",
    "After this there is a print statement to create a header for our top ten words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'frank': 1, 'n': 1, 'stein': 1, '': 4, 'education': 1, 'data': 7, 'analytics': 3, 'visualization': 2, 'boot': 1, 'camp': 1, 'graduate': 1, 'experience': 1, 'creating': 1, 'pivot': 1, 'tables': 1, 'vba': 1, 'scripts': 2, 'excel': 2, 'modeling': 1, 'forecasting': 1, 'basic': 1, 'statistics': 2, 'writing': 1, 'python': 4, 'analyze': 1, 'sets': 1, 'from': 1, 'files': 1, 'apis': 1, 'social': 2, 'media': 2, 'mining': 2, 'mysql': 1, 'mongodb': 1, 'databases': 1, 'developing': 1, 'frontend': 1, 'web': 2, 'visualizations': 1, 'html': 2, 'css': 2, 'bootstrap': 1, 'd3': 2, 'leafletjs': 1, 'the': 2, 'tableau': 2, 'business': 1, 'intelligence': 1, 'software': 2, 'performing': 1, 'big': 2, 'hadoop': 2, 'machine': 2, 'learning': 2, 'algorithms': 1, 'skills': 1, 'microsoft': 1, 'javascript': 2, 'htmlcss': 1, 'api': 1, 'interactions': 1, 'sql': 1, 'advanced': 1, 'r': 1, 'gitgithub': 1, 'interests': 1, 'contributing': 1, 'opensource': 1, 'pandas': 1, 'designing': 1, 'apps': 1, 'cloud': 1, 'aws': 1}\n",
      "Counter({'data': 7, '': 4, 'python': 4, 'analytics': 3, 'visualization': 2, 'scripts': 2, 'excel': 2, 'statistics': 2, 'social': 2, 'media': 2, 'mining': 2, 'web': 2, 'html': 2, 'css': 2, 'd3': 2, 'the': 2, 'tableau': 2, 'software': 2, 'big': 2, 'hadoop': 2, 'machine': 2, 'learning': 2, 'javascript': 2, 'frank': 1, 'n': 1, 'stein': 1, 'education': 1, 'boot': 1, 'camp': 1, 'graduate': 1, 'experience': 1, 'creating': 1, 'pivot': 1, 'tables': 1, 'vba': 1, 'modeling': 1, 'forecasting': 1, 'basic': 1, 'writing': 1, 'analyze': 1, 'sets': 1, 'from': 1, 'files': 1, 'apis': 1, 'mysql': 1, 'mongodb': 1, 'databases': 1, 'developing': 1, 'frontend': 1, 'visualizations': 1, 'bootstrap': 1, 'leafletjs': 1, 'business': 1, 'intelligence': 1, 'performing': 1, 'algorithms': 1, 'skills': 1, 'microsoft': 1, 'htmlcss': 1, 'api': 1, 'interactions': 1, 'sql': 1, 'advanced': 1, 'r': 1, 'gitgithub': 1, 'interests': 1, 'contributing': 1, 'opensource': 1, 'pandas': 1, 'designing': 1, 'apps': 1, 'cloud': 1, 'aws': 1})\n",
      "{'frank': 1, 'n': 1, 'stein': 1, 'education': 1, 'data': 7, 'analytics': 3, 'visualization': 2, 'boot': 1, 'camp': 1, 'graduate': 1, 'experience': 1, 'creating': 1, 'pivot': 1, 'tables': 1, 'vba': 1, 'scripts': 2, 'excel': 2, 'modeling': 1, 'forecasting': 1, 'basic': 1, 'statistics': 2, 'writing': 1, 'python': 4, 'analyze': 1, 'sets': 1, 'from': 1, 'files': 1, 'apis': 1, 'social': 2, 'media': 2, 'mining': 2, 'mysql': 1, 'mongodb': 1, 'databases': 1, 'developing': 1, 'frontend': 1, 'web': 2, 'visualizations': 1, 'html': 2, 'css': 2, 'bootstrap': 1, 'd3': 2, 'leafletjs': 1, 'the': 2, 'tableau': 2, 'business': 1, 'intelligence': 1, 'software': 2, 'performing': 1, 'big': 2, 'hadoop': 2, 'machine': 2, 'learning': 2, 'algorithms': 1, 'skills': 1, 'microsoft': 1, 'javascript': 2, 'htmlcss': 1, 'api': 1, 'interactions': 1, 'sql': 1, 'advanced': 1, 'r': 1, 'gitgithub': 1, 'interests': 1, 'contributing': 1, 'opensource': 1, 'pandas': 1, 'designing': 1, 'apps': 1, 'cloud': 1, 'aws': 1}\n",
      "False\n",
      "Top 10 Words\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "# Resume Word Count\n",
    "# ==========================\n",
    "# Initialize a dictionary with default values equal to zero\n",
    "word_count = {}.fromkeys(word_list, 0)\n",
    "\n",
    "# Loop through the word list and count each word.\n",
    "for word in word_list:\n",
    "    word_count[word] += 1\n",
    "    \n",
    "print(word_count)\n",
    "\n",
    "# Bonus using collections.Counter\n",
    "word_counter = Counter(word_list)\n",
    "print(word_counter)\n",
    "word_count.pop(\"\")\n",
    "print(word_count)\n",
    "\n",
    "# Comparing both word count solutions\n",
    "print(word_count == word_counter)\n",
    "\n",
    "# Top 10 Words\n",
    "print(\"Top 10 Words\")\n",
    "print(\"=============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we are first creating an empty list named 'sorted_words'. We then run a for loop which sorts the word_count dictionary based on its values column and then returns them in reverse order (python sorts ascending by default) in order to get the top ten variables. The '[:10]' tells python to return the first 10 items from the list. We then run a print statement in the for loop to display the items in descneding order. The print statement specifies that it will return the key first, since \"word\" is our iterator that we are usign to iterate though the keys and then it uses ':20' to put 20 spaces between the end of \"Token:\" and \"Count\" so that the list is formatted nicely. \n",
    "\n",
    "We dont need to get rid of the blank token here because it was removed from the word_count dicitonary in the last cell by using 'pop'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: data                 Count: 7\n",
      "Token: python               Count: 4\n",
      "Token: analytics            Count: 3\n",
      "Token: visualization        Count: 2\n",
      "Token: scripts              Count: 2\n",
      "Token: excel                Count: 2\n",
      "Token: statistics           Count: 2\n",
      "Token: social               Count: 2\n",
      "Token: media                Count: 2\n",
      "Token: mining               Count: 2\n"
     ]
    }
   ],
   "source": [
    "# Sort words by count and print the top 10\n",
    "sorted_words = []\n",
    "\n",
    "for word in sorted(word_count, key=word_count.get, reverse=True)[:10]:\n",
    "    print(f\"Token: {word:20} Count: {word_count[word]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
